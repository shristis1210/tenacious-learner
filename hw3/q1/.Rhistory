print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvl$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvl$lambda.1se  # lambda for this MSE
## [1] 0.01516562
#fitted coefficients at minimum MSE
coef(cvl, select="min")
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
## [1] 0.03585894
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
## [1] 0.0828388
#fitted coefficients at minimum MSE
coef(cvr, select="min")
## residual bootstrap Lasso
set.seed(0)
obj <- bootLasso(x = x, y = y, B = 10)
# confidence interval
obj$interval
#optimal lambda
obj$lambda.opt
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
e^(0.019)
exp(0.019)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
# import data and examine it
greenbuildings <- read.csv("~/GitHub/SDS323_Spring2020/hw3/q1/greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvl$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvl$lambda.1se  # lambda for this MSE
## [1] 0.01516562
#fitted coefficients at minimum MSE
coef(cvl, select="min")
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
## [1] 0.03585894
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
## [1] 0.0828388
#fitted coefficients at minimum MSE
coef(cvr, select="min")
## residual bootstrap Lasso
set.seed(0)
obj <- bootLasso(x = x, y = y, B = 10)
# confidence interval
obj$interval
#optimal lambda
obj$lambda.opt
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
# Assessing Outliers
outlierTest(linear_fit) # Bonferonni p-value for most extreme obs
qqPlot(linear_fit, main="QQ Plot") #qq plot for studentized resid
leveragePlots(linear_fit) # leverage plots
setwd("~/Google Drive/UT Austin courses/SDS323_Spring2020/hw3/q1")
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvl$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvl$lambda.1se  # lambda for this MSE
## [1] 0.01516562
#fitted coefficients at minimum MSE
coef(cvl, select="min")
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
## [1] 0.03585894
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
## [1] 0.0828388
#fitted coefficients at minimum MSE
coef(cvr, select="min")
## residual bootstrap Lasso
set.seed(0)
obj <- bootLasso(x = x, y = y, B = 10)
# confidence interval
obj$interval
#optimal lambda
obj$lambda.opt
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
# Assessing Outliers
outlierTest(linear_fit) # Bonferonni p-value for most extreme obs
qqPlot(linear_fit, main="QQ Plot") #qq plot for studentized resid
leveragePlots(linear_fit) # leverage plots
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
# Here I fit my lasso regression to the data and do my cross validation of k=10 n folds
# the cv.gamlr command does both things at once.
#(verb just prints progress)
cvl = cv.gamlr(x, y, nfold=10, verb=TRUE)
# plot the out-of-sample deviance as a function of log lambda
plot(cvl, bty="n")
min(cvl$cvm)       # minimum MSE
## [1] 0.06615445
cvl$lambda.min     # lambda for this min MSE
## [1] 0.003585894
cvl$cvm[cvl$lambda == cvl$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvl$lambda.1se  # lambda for this MSE
## [1] 0.01516562
#fitted coefficients at minimum MSE
coef(cvl, select="min")
# Apply CV Ridge regression to data
cvr <- cv.glmnet(
x ,
y ,
alpha = 0
)
# plot MSE as a function of log(lambda)
plot(cvr)
min(cvr$cvm)       # minimum MSE
## [1] 0.06679016  #value observed
cvr$lambda.min     # lambda for this min MSE
## [1] 0.03585894
cvr$cvm[cvr$lambda == cvr$lambda.1se]  # 1 st.error of min MSE
## [1] 0.06908108
cvr$lambda.1se  # lambda for this MSE
## [1] 0.0828388
#fitted coefficients at minimum MSE
coef(cvr, select="min")
## residual bootstrap Lasso
set.seed(0)
obj <- bootLasso(x = x, y = y, B = 10)
# confidence interval
obj$interval
#optimal lambda
obj$lambda.opt
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings, par(mfrow = c(2, 2))) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
# Assessing Outliers
outlierTest(linear_fit) # Bonferonni p-value for most extreme obs
qqPlot(linear_fit, main="QQ Plot") #qq plot for studentized resid
leveragePlots(linear_fit) # leverage plots
install.packages("olsrr")
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
library(olsrr)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
ols_plot_resid_qq(model)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
library(olsrr)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
ols_plot_resid_qq(linear)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
library(olsrr)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
ols_plot_resid_qq(linear_fit)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
library(olsrr)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
ols_plot_resid_qq(linear_fit)
ols_test_normality(linear_fit)
ols_test_correlation(linear_fit)
library(rsample)  # data splitting
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
library(DAAG)
library(MASS)
library("mvtnorm")
library(HDCI)
library(olsrr)
# import data and examine it
greenbuildings <- read.csv("greenbuildings.csv")
#View(greenbuildings)
ok <- complete.cases(greenbuildings)
greenbuildings <- greenbuildings[ok,]
# note that shares is hugely skewed
# probably want a log transformation here
hist(greenbuildings$Rent)
summary(greenbuildings$Rent)
# much nicer :-)
hist(log(greenbuildings$Rent))
#### lasso (glmnet does L1-L2, gamlr does L0-L1)
# I want to fit a lasso regression and do cross validation of K=10 folds
# inorder to automate finiding independent variables and training & testing my data multiple times.
# cv.gamlr command in the gamlr does it for me.
# download gamlr library
library(gamlr)
# i create a matrix of all my independent varaibles except for url from online_news data to make it easily readable for gamlr commands.
# the sparse.model.matrix function.
x = sparse.model.matrix( log(Rent) ~  . - CS_PropertyID - LEED -Energystar  , data=greenbuildings, standardize=TRUE)[, -1] # do -1 to drop intercep
y = log(greenbuildings$Rent) # pull out `y' too just for convenience and do log(shares)- dependent variable
#Apply OLS to data
linear_fit = lm(log(Rent) ~ . - CS_PropertyID - LEED -Energystar , data = greenbuildings) #no scaling  in linear model, need to include intercept term
cvlm = cv.lm(data = greenbuildings, linear_fit, m=10, plotit = FALSE, printit = FALSE)
print(linear_fit)
#MSE for OLS = 0.0659
confint(linear_fit)
ols_plot_resid_qq(linear_fit)
ols_test_correlation(linear_fit)
ols_plot_resid_fit(linear_fit)
